# Современные языковые модели: от предсказания слов до агентного поведения (короткий обзор) [Расширенная лекция]([https://example.com](https://submaroon.github.io/Ai-bases/))

Современные генеративные языковые модели (LLM), такие как GPT, LLaMA, Claude и другие, представляют собой мощные инструменты, способные генерировать человекоподобный текст. На самом высоком уровне их можно рассматривать как "черные ящики": на вход подается текст, и на выходе также получается текст. Но что происходит внутри этих моделей и как они обучаются?

## Основы обучения: предсказание следующего токена

Большую часть времени языковая модель обучается предсказывать следующее слово или, точнее, следующий *токен* (часть слова) в последовательности. Модель анализирует огромные объемы текстовых данных, запоминая статистические закономерности. Например, если в обучающих данных часто встречается фраза "я обычно иду гулять в парк", модель запомнит, что после "обычно" часто следует "иду", а после "иду" – "в парк". Это похоже на продвинутый автокомплит.

Этот процесс можно сравнить с Марковским процессом, где следующее состояние зависит только от текущего. Однако нейросети учитывают контекст – определенное количество предыдущих токенов. Если контекстное окно модели составляет, например, 8192 токена, то для предсказания следующего токена она будет анализировать эти последние 8192 токена. Генерация текста происходит пошагово: предсказывается один токен, он добавляется к контексту, контекст сдвигается, и процесс повторяется. Важно понимать, что на этом фундаментальном этапе модель не "понимает" текст в человеческом смысле, а скорее учится вероятностям совместного появления токенов.

## Внутренняя модель мира и интерпретируемость

Хотя базовый принцип – предсказание следующего токена – кажется простым, исследования показывают, что внутри LLM формируется некая *модель окружающего мира*. Примером может служить исследование "Othello-GPT", где модель обучали на записях партий в игру Отелло. Выяснилось, что из внутренних активаций нейросети (конкретно, декодера-трансформера) можно восстановить позиции всех фигур на доске. Более того, изменение этих активаций для имитации другого расположения фигур приводило к корректному и часто оптимальному предсказанию следующего хода. Это указывает на то, что модель не просто запомнила последовательности ходов, а выучила некие правила и репрезентации игрового состояния.

Это говорит о том, что LLM не просто "стохастические попугаи", повторяющие заученные фразы. Они обладают способностью к некоторой генерализации – принятию осмысленных решений на данных, которые они не видели ранее. Однако степень этой генерализации ограничена. Модель, не обучавшаяся на коде (или на специфическом языке программирования, например, 1С, как упоминалось в контексте проекта GigaCode), будет плохо программировать на нем. Качество генерации сильно зависит от того, какие данные были в обучающей выборке.

Понимание того, как именно работают нейросети, – это предмет активных исследований в области *интерпретируемости*. Мы создаем сложные системы, которые выдают результат, но точные механизмы их "мышления" не всегда до конца ясны. Это большое и важное поле исследований, пытающееся объяснить, почему модель принимает те или иные решения.

## Данные, токены и масштабирование

Для обучения LLM требуются огромные объемы данных. Современные модели, такие как Qwen, обучаются на десятках триллионов токенов. Существуют так называемые *законы масштабирования* (scaling laws), которые пытаются определить оптимальное количество данных. Одна из ранних эвристик (Chinchilla scaling laws) предполагала около 20 токенов на каждый параметр модели. Например, для модели с миллиардом параметров потребовалось бы 20 миллиардов токенов. Однако на практике сейчас используется больше данных, так как это обычно приводит к улучшению качества, и эксперименты в этой области очень дороги, что затрудняет точное определение оптимальных соотношений. Пока неясно, существует ли порог, после которого добавление данных перестает давать значимый прирост качества для очень больших моделей (например, более 7 миллиардов параметров).

Теоретический предел сжатия информации описывается **энтропией Шеннона**. Формула для энтропии источника информации $H$ с $N$ возможными сообщениями, где каждое сообщение $x_i$ имеет вероятность $p_i$, выглядит так:

$$H = -\sum_{i=1}^{N} p_i \log_b p_i$$

Где $b$ – основание логарифма (обычно 2 для битов). Это означает, что мы не можем сжать данные без потерь сильнее определенного порога. Трансформеры, однако, демонстрируют удивительную способность запоминать огромные объемы информации, и пока для больших моделей мы не можем "нащупать" точку, где добавление данных перестает улучшать качество.

### Что такое токен?

Текст для модели представляется в виде последовательности *токенов*. Токен – это минимальная смысловая единица для модели, чаще всего часть слова или целое короткое слово. Процесс разбиения текста на токены называется *токенизацией*, а сам инструмент – *токенизатором*. Каждая модель имеет свой токенизатор.

Один из популярных алгоритмов токенизации – **Byte Pair Encoding (BPE)**. Процесс его создания выглядит примерно так:
1.  Изначально словарь токенов состоит из всех отдельных символов (например, букв алфавита), встречающихся в обучающем корпусе.
2.  Затем итеративно анализируется корпус данных: находится наиболее часто встречающаяся пара соседних токенов (например, "А" и "Б").
3.  Эта пара объединяется в новый, более длинный токен ("АБ"), который добавляется в словарь.
4.  Этот процесс повторяется: на следующих шагах могут объединяться уже существующие многобуквенные токены с однобуквенными или другими многобуквенными (например, "АБ" и "В" в "АБВ").
5.  Объединение продолжается до тех пор, пока размер словаря не достигнет заданного предела (например, 50 000 или 100 000 токенов) или пока не останется достаточно частых пар для объединения. В словарь добавляются самые популярные комбинации.

Качество токенизации зависит от языка и данных, на которых обучался токенизатор. Если в обучающих данных мало текстов на определенном языке (например, русского языка в ранних версиях LLaMA), токенизация для этого языка может быть неэффективной: одно русское слово может разбиваться на множество коротких токенов (иногда даже на отдельные буквы), что увеличивает длину последовательности и снижает качество работы модели. Современные модели, такие как GPT-4 или Gemini, имеют более эффективные токенизаторы для многих языков, где на одно слово приходится в среднем 2-3 токена. Существуют и другие методы токенизации, например, те, что позволяют моделям работать с необычными символами, такими как Zalgo-текст, анализируя внутреннюю структуру UTF-кодировки.

## Этапы обучения языковых моделей

Обучение современных LLM обычно проходит в несколько этапов:

1.  **Pre-training (Предварительное обучение):** Модель обучается на огромных массивах неструктурированного текста (весь интернет, книги и т.д.) предсказывать следующий токен. На этом этапе закладываются базовые языковые знания и формируется некоторая "модель мира". Для адаптации модели, уже видевшей язык на претрейне, к специфике языка (например, русскому) может потребоваться несколько миллиардов, а лучше несколько десятков миллиардов токенов.
2.  **Supervised Fine-Tuning (SFT) / Instruction Tuning (Обучение на инструкциях):** Модель дообучается на структурированных данных в формате "вопрос-ответ" или "инструкция-выполнение". Для этого используются специальные *chat-темплейты* – шаблоны, которые показывают модели, как должен выглядеть диалог, разделяя реплики пользователя, ассистента, а иногда и системные инструкции или контекстные документы. Например:
    ```
    <|system|>
    Ты полезный ассистент.
    <|user|>
    Какой сегодня день?
    <|assistant|>
    Сегодня [дата].
    ```
    Модель учится следовать этому формату и отвечать на запросы пользователя. Изначально датасеты для SFT могли составляться вручную или с помощью "few-shot" промптинга уже существующих больших моделей (как в GPT-2, где модель могла выполнять простые задачи после нескольких примеров в промпте без дополнительного обучения). Один из ранних популярных датасетов инструкций назывался "Flan", но сейчас он считается устаревшим. Позже появились датасеты, частично собранные вручную (например, Stanford Alpaca), а частично – с помощью более крупных моделей вроде GPT-3.5, которые генерировали ответы на заданные вопросы (синтетические данные). Этот процесс, когда большая модель "обучает" меньшую, называется *дистилляцией* (в данном случае, "hard label distillation", когда маленькая модель учится на конкретных ответах большой).
    Были и скандалы, связанные с привлечением низкооплачиваемых работников (например, в Кении) для разметки данных для OpenAI, хотя условия их труда, по некоторым данным, были лучше средних по региону.
3.  **Alignment (Выравнивание) / Preference Tuning (Обучение на предпочтениях):** Этот этап (также называемый RLHF/RLAIF, DPO, KTO и т.д.) направлен на то, чтобы ответы модели соответствовали человеческим ожиданиям по полезности, правдивости и безвредности. Обычно это самый короткий этап, требующий относительно небольшого датасета (например, 5-10 тысяч примеров).
    * **RLHF (Reinforcement Learning from Human Feedback):** Сначала обучается "модель вознаграждения" (reward model), которая оценивает, насколько хорош ответ LLM с точки зрения человека (например, насколько он полезен, нетоксичен). Затем основная LLM дообучается с помощью методов обучения с подкреплением, чтобы генерировать ответы, максимизирующие оценку модели вознаграждения.
    * **DPO (Direct Preference Optimization):** Модель напрямую обучается на парах ответов, где один помечен как предпочтительный, а другой – как менее предпочтительный, без необходимости в отдельной модели вознаграждения.

Для удаления нежелательного поведения (например, генерации токсичного контента или отказов отвечать) могут применяться техники вроде *облитерации*. Это подход, предложенный исследователями (в частности, из сообщества LessWrong), который заключается в поиске и модификации определенных паттернов во внутренних активациях модели, отвечающих за нежелательное поведение. Вместо того чтобы "переучивать" модель на "правильные" ответы, что может повредить ее знаниям, облитерация пытается "стереть" или "ослепить" модель относительно конкретных нежелательных концепций.

## Галлюцинации и борьба с ними

Одной из главных проблем LLM являются *галлюцинации* – генерация правдоподобной, но фактически неверной или вымышленной информации. Это происходит потому, что модель всегда пытается продолжить текст, даже если не знает ответа. Ее не учили говорить "я не знаю", так как в обучающих данных (например, текстах из интернета) люди редко признаются в незнании. Модель статистически выводит наиболее вероятное продолжение, которое может быть фактической ошибкой (например, неверная длина реки).

Полностью избавиться от галлюцинаций пока невозможно, но существуют способы их уменьшения:

* **Retrieval Augmented Generation (RAG):** Это подход, при котором перед генерацией ответа модель получает релевантную информацию из внешней базы знаний.
    1.  Запрос пользователя сначала используется для поиска информации в базе данных (например, Wikipedia, корпоративные документы).
    2.  Найденные документы (чанки текста) добавляются в контекст LLM вместе с исходным запросом.
    3.  Модель генерирует ответ, опираясь на предоставленные факты.

    Для поиска в RAG часто используются *эмбеддеры* – модели, преобразующие текст в числовые векторы (эмбеддинги). Близость векторов указывает на семантическую схожесть текстов. База знаний предварительно индексируется: тексты разбиваются на чанки, каждый чанк преобразуется в эмбеддинг и сохраняется. При поступлении запроса он также преобразуется в эмбеддинг, и в базе находятся наиболее близкие по векторному расстоянию чанки. Эмбеддеры обычно значительно меньше по размеру (десятки-сотни миллионов параметров), чем генеративные модели.

    Помимо векторного поиска, могут использоваться и другие методы, например, **BM25 (Best Matching 25)**. Это алгоритм ранжирования, основанный на статистике встречаемости слов (TF-IDF подобный), который хорошо работает для поиска по ключевым словам и точным совпадениям (например, аббревиатур). Формула BM25 для оценки релевантности документа $D$ запросу $Q$, состоящему из слов $q_1, ..., q_n$:

    $$\text{score}(D, Q) = \sum_{i=1}^{n} \text{IDF}(q_i) \cdot \frac{f(q_i, D) \cdot (k_1 + 1)}{f(q_i, D) + k_1 \cdot (1 - b + b \cdot \frac{|D|}{\text{avgdl}})}$$

    Где:
    * $f(q_i, D)$ – частота слова $q_i$ в документе $D$.
    * $|D|$ – длина документа $D$ (количество слов).
    * avgdl – средняя длина документа в коллекции.
    * $k_1$ и $b$ – свободные коэффициенты (обычно $k_1 \in [1.2, 2.0]$, $b = 0.75$).
    * $\text{IDF}(q_i)$ – инверсная документная частота слова $q_i$.

    Часто используется гибридный подход, сочетающий векторный поиск и BM25, а также последующее ранжирование найденных результатов с помощью *ре-ранкеров* перед подачей в LLM. Однако RAG не является панацеей: модель все еще может неверно интерпретировать найденный текст или получить нерелевантные документы, если поисковый компонент слаб.
* **Обучение говорить "я не знаю":** Это активная область исследований, но пока нет универсального решения.
* **Обнаружение галлюцинаций:** Разрабатываются методы, позволяющие выявлять моменты, когда модель неуверена в своем ответе или генерирует информацию, противоречащую известным фактам.

## Архитектура Трансформера

В основе большинства современных LLM лежит архитектура **Трансформер**, предложенная в статье "Attention Is All You Need" (Vaswani et al., 2017). Ключевым элементом трансформера является механизм *внимания (attention)*.

1.  **Входные данные (токены)** преобразуются в векторы (эмбеддинги) с помощью обучаемого *слоя эмбеддингов (embedding layer)*. Этот слой, по сути, является словарем, который сопоставляет каждому ID токена плотный вектор фиксированной размерности.
2.  Эти векторы проходят через несколько одинаковых блоков (слоев) трансформера.
3.  Каждый блок состоит из двух основных подслоев:
    * **Multi-Head Attention (Многоголовочное внимание):** Позволяет модели взвешивать важность различных частей входной последовательности при обработке каждой ее части. Для каждого токена из его эмбеддинга создаются три вектора: *Query (Q, Запрос)*, *Key (K, Ключ)* и *Value (V, Значение)* путем умножения на обучаемые матрицы весов $W_Q, W_K, W_V$.
        * Оценка внимания (сходства) между Query токеном $i$ и Key токеном $j$ вычисляется как скалярное произведение $Q_i \cdot K_j^T$.
        * Эти оценки нормализуются (делятся на $\sqrt{d_k}$, где $d_k$ – размерность векторов K) и пропускаются через функцию Softmax для получения весов внимания. Эти веса показывают, насколько каждый токен во входной последовательности "важен" для текущего токена.
        * Выходной вектор для токена $i$ – это взвешенная сумма векторов Value всех токенов.
        Формула для одного "головы" внимания:
        $$\text{Attention}(Q, K, V) = \text{softmax}\left(\frac{QK^T}{\sqrt{d_k}}\right)V$$
        В многоголовочном внимании этот процесс выполняется параллельно несколькими "головами" с разными матрицами весов, результаты конкатенируются и снова пропускаются через линейный слой. Это позволяет модели одновременно фокусироваться на разных аспектах информации.
    * **Feed-Forward Network (FFN, Полносвязная сеть прямого распространения):** Применяется к каждому токену независимо. Обычно состоит из двух линейных слоев с функцией активации (например, ReLU или GeLU) между ними. Считается, что этот слой отвечает за хранение и извлечение "знаний" модели, выученных на этапе pre-training.

4.  Вокруг каждого из этих двух подслоев используются *остаточные соединения (residual connections)* и *нормализация слоев (layer normalization)*. Остаточное соединение означает, что вход подслоя добавляется к его выходу ($X + \text{Sublayer}(X)$), что помогает обучать глубокие сети, предотвращая затухание градиента. Данные как бы проходят через "главный поток" (residual stream), и каждый блок добавляет к этому потоку свою информацию.
5.  На выходе последнего блока трансформера получается последовательность векторов, которая затем используется для предсказания следующего токена (например, через линейный слой и Softmax).

Обучение трансформера, как и других нейросетей, происходит с помощью *градиентного спуска* и алгоритма *обратного распространения ошибки (backpropagation)*. Модель делает предсказание, вычисляется ошибка (loss function – разница между предсказанным и реальным следующим токеном), и затем веса модели (миллиарды параметров, которые являются коэффициентами в огромном уравнении модели) корректируются в направлении, уменьшающем эту ошибку. Скорость и направление изменения весов определяются *градиентом* и *скоростью обучения (learning rate)*.

## Использование инструментов (Tool Use) и Агентность

LLM могут не только генерировать текст, но и взаимодействовать с внешними инструментами. В статье "ToolFormer" было показано, как обучить модель вызывать API, например, калькулятора. Если модель встречает задачу "сколько будет 5+5?", она может сгенерировать специальный токен или структурированный вызов (например, JSON), указывающий на вызов функции калькулятора с аргументами 5 и 5. Внешняя система (оркестратор) выполняет вызов, получает результат (10) и передает его обратно модели, которая затем включает его в свой ответ.

Это открывает путь к *агентному поведению*, когда LLM может выполнять сложные задачи, разбивая их на шаги и используя различные инструменты (поиск в интернете, выполнение кода Python, доступ к базам данных, вызов API погоды и т.д.).

### System 1 vs System 2 Thinking, Chain-of-Thought и ReAct

Человеческое мышление часто делят на "Систему 1" (быстрое, интуитивное) и "Систему 2" (медленное, аналитическое, требующее рассуждений). Современные LLM (например, GPT-4, Claude 3, Gemini) начинают имитировать "Систему 2", генерируя цепочку рассуждений (*Chain-of-Thought, CoT*) перед тем, как дать окончательный ответ. Модель обучается сначала "подумать вслух", описать шаги решения, и только потом выдать результат. Это может улучшить качество ответов на сложные логические, математические или многоэтапные задачи. Пока не до конца понятно, почему это работает лучше – возможно, это заставляет модель активировать более релевантные "знания" или просто генерировать более длинный и когерентный текст. Иногда модели могут генерировать "мысли" даже с ошибками или нерелевантной информацией, но итоговый ответ все равно оказывается лучше.

Фреймворк **ReAct (Reasoning and Acting)**, предложенный в статье "ReAct: Synergizing Reasoning and Acting in Language Models", объединяет рассуждения и действия. Модель на каждом шаге генерирует:
1.  **Thought (Мысль):** Анализ текущей ситуации и план следующего действия.
2.  **Act (Действие):** Вызов инструмента (с определенными параметрами) или выдача финального ответа.
3.  **Observation (Наблюдение):** Результат выполнения действия (ответ от инструмента).

Этот итеративный процесс (Thought -> Act -> Observation -> Thought ...) позволяет моделям решать более комплексные задачи, динамически планируя свои действия и реагируя на новую информацию. Например, для ответа на вопрос "Что я могу приготовить на ужин, учитывая содержимое моего холодильника (фото) и мой бюджет?" модель может:
1.  (Thought) Нужно проанализировать фото холодильника. (Act) Вызвать инструмент анализа изображений с переданным фото.
2.  (Observation) Получен список продуктов: [помидоры, огурцы, сыр]. (Thought) Теперь нужно проверить бюджет и найти рецепты с этими продуктами. (Act) Вызвать инструмент поиска рецептов с параметрами: продукты=[помидоры, огурцы, сыр], бюджет=[сумма].
3.  (Observation) Получен список рецептов: [салат "Греческий", бутерброды]. (Thought) Рецепты найдены, можно сформулировать ответ. (Act) Выдать финальный ответ: "Вы можете приготовить салат 'Греческий' или бутерброды".

Хотя такие подходы увеличивают время ответа и затраты на вычисления (требуется несколько вызовов модели и инструментов), они значительно расширяют возможности LLM. Примером реализации такого агента может служить бот для поддержки студентов, который сначала ищет информацию в базе знаний, а затем отвечает на вопрос. Однако, как отмечалось в лекции, технологии быстро меняются, и код, написанный даже год назад, может потребовать обновления используемых моделей и подходов.

## Заключение

Современные языковые модели прошли долгий путь от простого предсказания следующего слова до способности рассуждать, использовать инструменты и решать сложные задачи. Понимание их архитектуры, процессов обучения и ограничений является ключом к их эффективному применению и дальнейшему развитию. Несмотря на впечатляющие успехи, такие проблемы, как галлюцинации, полная интерпретируемость и этические аспекты создания и использования ИИ, остаются открытыми областями для исследований и активных дискуссий.
